{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Workflow and Project Management with NOMAD","text":""},{"location":"#what-you-will-learn","title":"\ud83e\udded What You Will Learn","text":"<ul> <li>Organize and manage complex research workflows using NOMAD</li> <li>Integrate diverse data sources into a single, reproducible project</li> <li>Track data provenance and metadata</li> <li>Interface with the NOMAD repository programmatically for automation and high-throughput use</li> </ul>"},{"location":"#prerequisites","title":"\ud83d\udccc Prerequisites","text":"<p>This tutorial supports both graphical (GUI) and programmatic (Python/CLI) workflows.</p>"},{"location":"#required","title":"Required","text":"<ul> <li>\ud83c\udf10 NOMAD account   Free registration is required to upload and manage data</li> </ul>"},{"location":"#recommended-for-efficiency-and-automation","title":"Recommended (for efficiency and automation)","text":"<ul> <li> <p>\ud83d\udcbb Terminal environment   Install the workflow utility module via Bash (Linux/macOS) or PowerShell (Windows)</p> </li> <li> <p>\ud83d\udc0d Basic Python knowledge   Utilize workflow utility tools using provided Jupyter notebooks</p> </li> </ul> <p>Can't code? No problem.</p> <p>You can complete the tutorial entirely using the :</p> <ul> <li>Use provided templates to create input files manually</li> <li>Upload and manage workflows via NOMAD's web interface</li> <li>Skip all Python- and Jupyter-related steps</li> </ul> <p>\u26a0\ufe0f Some tasks may be more manual and time-consuming, but all core functionality is accessible without programming.</p>"},{"location":"#tutorial-preparation","title":"\u2699\ufe0f Tutorial Preparation","text":""},{"location":"#1-create-a-nomad-account-at-nomad-test-deployment","title":"1. Create a NOMAD account at NOMAD Test Deployment","text":"<p>Click <code>LOGIN/REGISTER</code> at the top right.</p> <p>Attention</p> <p>This entire tutorial will us the test deployment. When you are ready to upload and share your real data with the public, use the NOMAD Central Deployment.</p>"},{"location":"#2-install-the-nomad-utility-workflows-module","title":"2. Install the nomad-utility-workflows module","text":"<p>Open a terminal and create a virtual environment with python==3.11 (It may be possible to use python&gt;=3.9, but the module has only been fully tested with 3.11):</p> macOS and LinuxWindows PowerShell <pre><code>python3.11 -m venv .pyenv\n</code></pre> <pre><code>py -3.11 -m venv .pyenv\n</code></pre> Install missing Python 3.11 interpreter <p>To install Python 3.11 interpreter:</p> Debian Linux <pre><code>sudo apt install python3.11\n</code></pre> Red Hat Linux <pre><code>sudo dnf install python3.11\n</code></pre> macOS <pre><code>brew install python@3.11\n</code></pre> Windows PowerShell <p>Download the installer from the official Python website and run it. Make sure to check the box that says \"Add Python 3.11 to PATH\" during installation.</p> <p>Activate the Python virtual environment:</p> macOS and LinuxWindows PowerShell <pre><code>. .pyenv/bin/activate\n</code></pre> <pre><code>.pyenv\\Scripts\\activate\n</code></pre> <p>Upgrade pip and install uv (recommended):</p> <pre><code>pip install --upgrade pip\npip install uv\n</code></pre> <p>Install the latest pypi version of the plugin using pip:</p> <pre><code>uv pip install \"nomad-utility-workflows[vis]&gt;=0.1.0\"\n</code></pre> <p>Install <code>python-dotenv</code> package. In order to use a Jupyter notebook in the following, install ipython and then create a Jupyter kernel for this venv (this kernel can then be be identified and loaded into your IDE):</p> <pre><code>uv pip install python-dotenv\nuv pip install --upgrade ipython\nuv pip install --upgrade ipykernel\nipython kernel install --user --name=nomad-tutorial-workflows\n</code></pre> <p>Now you should be able to simply launch a Jupyter notebook browser with <code>jupyter notebook</code> in the terminal. Open a (new) <code>.ipynb</code> file, and then select <code>nomad-tutorial-workflows</code> from the kernel list.</p>"},{"location":"#resources","title":"Resources","text":""},{"location":"#1-explore-the-nomad-documentation","title":"1. Explore the NOMAD documentation.","text":""},{"location":"#2-join-our-vibrant-discord-community-invitation-to-discord","title":"2. Join our vibrant Discord community! Invitation to Discord","text":""},{"location":"api/","title":"Part 2: Using NOMAD\u2019s API for Project Management","text":""},{"location":"api/#what-you-will-learn","title":"\ud83c\udfaf What You Will Learn","text":"<ul> <li>How to interface with NOMAD using a simplified Python API</li> <li>How to (programmatically) upload data, edit metadata, create datasets, and publish data</li> </ul>"},{"location":"api/#programmatic-uploads","title":"\ud83d\udee0\ufe0f Programmatic Uploads","text":"<p>Challenge: You need to upload the DFT calculations to NOMAD, but this large set of calculations were performed and are currently stored on your groups HPC cluster. (Here we will only work with 3 calculations for demonstration purposes).</p> <p>Your Solution: Use the <code>nomad-utility-workflows</code> module!</p> <p>Attention</p> <p>The exercises below are intended to be performed in a single jupyter session (pre-filled example notebooks provided below).</p>"},{"location":"api/#set-up-files-and-data","title":"Set up / files and data","text":"<p>On the HPC cluster we have collected our data within the following directory structure:</p> <pre><code>root\n\u251c\u2500\u2500 DFT-1\n\u2502   \u2514\u2500\u2500 0iCl0nWwCftF0tgQOaklcQLFB68E.zip\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 aims.out # FHIAims mainfile\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 ...other raw simulation files\n\u251c\u2500\u2500 DFT-2\n\u2502   \u2514\u2500\u2500 24Q4MoaAUtsWN7Hepw3UH3TU93pX.zip\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 aims.out # FHIAims mainfile\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 ...other raw simulation files\n\u2514\u2500\u2500 DFT-3\n    \u2514\u2500\u2500 6V_q8X39he-dakYHifH_3Z53GTdZ.zip\n \u00a0\u00a0     \u251c\u2500\u2500 aims.out # FHIAims mainfile\n \u00a0\u00a0     \u2514\u2500\u2500 ...other raw simulation files\n</code></pre> <p>Note</p> <p>This data was actually obtained from querying NOMAD for DFT calculations of water, with a certain number of atoms within the simulations. The zip files are named according to the corresponding <code>entry_id</code>'s of this data. Instead of supplying the test data directly here, later in this part of the tutorial, we will instead use the API functionalities to reconstruct the above structure by downloading the data directly from NOMAD.</p>"},{"location":"api/#install-the-nomad-utility-workflows-plugin","title":"Install the nomad-utility-workflows plugin","text":"<p>If you have not done so already, follow the installation instructions Tutorial Preparation.</p>"},{"location":"api/#linking-to-your-nomad-account","title":"Linking to your NOMAD account","text":"<p>If you have not done so already, create a NOMAD account at NOMAD Central Deployment. Click <code>LOGIN/REGISTER</code> at the top right.</p> <p>Store your credentials in a file called <code>.env</code> in the root working directory for this tutorial with the following content: <pre><code>NOMAD_USERNAME=\"MyLogin\"\nNOMAD_PASSWORD=\"MyPassWord\"\n</code></pre> and insert your username and password.</p> <p>Attention</p> <p>The environment file MUST be called exactly \".env\" to be read in correctly with our approach.</p> <p>The <code>.env</code> file you placed in the root folder will be loaded within each jupyter notebook with the command <code>load_dotenv()</code> found in next steps. The functions within the utility module will automatically retrieve an authentication token for privileged operations, e.g., uploading data to your account.</p> Tip - If your <code>.env</code> file is not found <p>If the authentication credentials from your <code>.env</code> file is not picked, add this directory to your <code>PYTHONPATH</code>: <pre><code>export PYTHONPATH=$PYTHONPATH:&lt;path-to-tutorial-root-working-directory&gt;\n</code></pre></p>"},{"location":"api/#nomad-urls-deployments","title":"NOMAD URLs / Deployments","text":"<p>The central NOMAD services offer several different deployments with varying purposes and versioning:</p> <ul> <li>\"prod\": The official NOMAD deployment.<ul> <li>Updated most infrequently (as advertised in #software-updates on the NOMAD Discord Server\u2014If you are not yet a member of the NOMAD server use Invitation to Discord )</li> </ul> </li> <li>\"staging\": The beta version of NOMAD.<ul> <li>Updated more frequently than prod in order to integrate and test new features.</li> </ul> </li> <li>\"test\": A test NOMAD deployment. (Used in this tutorial).<ul> <li>The data is occassionally wiped, such that test publishing can be made.</li> </ul> </li> </ul> <p>Note that the \"prod\" and \"staging\" deployments share a common database, and that publishing on either will result in publically available data.</p> <p>All API functions in <code>nomad-utility-workflows</code> allow the user to specify the URL with the optional keyword <code>url</code>. If you want to use the central NOMAD URLs, you can simply set <code>url</code> equal to \"prod\", \"staging\", or \"test\". By default, the test deployment will be used as a safety mechanism to avoid accidentally publishing during testing. Thus, for all examples in this tutorial we will be using the test deployment.</p> <p>See nomad-utility-workflow DOCS &gt; NOMAD URLs for more information.</p>"},{"location":"api/#uploading-api-basics","title":"Uploading API Basics","text":"<p>To demonstrate the basics of the API functionalities within <code>nomad-utility-workflows</code>, let's upload some dummy data.</p> <p>You can create a <code>test-API.ipynb</code> notebook and copy over the following step by step, or download the prefilled notebook:</p> <p>Download test-API.ipynb</p> <p>Import the necessary modules/functions:</p> <pre><code>from dotenv import load_dotenv\nload_dotenv()\nimport os\nimport zipfile\nfrom pprint import pprint\nfrom nomad_utility_workflows.utils.uploads import (\nupload_files_to_nomad,\nget_upload_by_id,\ndelete_upload\n)\ndef create_zip(zip_name, dir_name):\nwith zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\nfor root, dirs, files in os.walk(dir_name):\nfor file in files:\nzipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), dir_name))\n</code></pre> <p>Create an empty folder and zip it:</p> <pre><code>os.makedirs('Test/', exist_ok=True)\ncreate_zip('test.zip', 'Test/')\ntest_upload_fnm = './test.zip'\nprint(f'Zip Created: {os.path.isfile(test_upload_fnm)}')\n</code></pre> <p>Upload the zip file to the test NOMAD deployment using the <code>upload_files_to_nomad()</code> function (here we will ensure that your authentication info is set up properly):</p> <pre><code>upload_id = upload_files_to_nomad(filename=test_upload_fnm, url='test')\nprint(upload_id)\n</code></pre> example output <pre><code>'RdA_3ZsOTMqbtAhYLivVsw'\n</code></pre>"},{"location":"api/#checking-the-upload-status","title":"Checking the upload status","text":"<p>The returned <code>upload_id</code> can then be used to directly access the upload, e.g., to check the upload status, using <code>get_upload_by_id()</code>:</p> <pre><code>nomad_upload = get_upload_by_id(upload_id, url='test')\npprint(nomad_upload)\n</code></pre> example output <pre><code>NomadUpload(upload_id='RdA_3ZsOTMqbtAhYLivVsw',\nupload_create_time=datetime.datetime(2024, 10, 15, 20, 2, 10, 378000),\nmain_author=NomadUser(name='Joseph Rudzinski'),\nprocess_running=False,\ncurrent_process='process_upload',\nprocess_status='SUCCESS',\nlast_status_message='Process process_upload completed successfully',\nerrors=[],\nwarnings=[],\ncoauthors=[],\ncoauthor_groups=[],\nreviewers=[],\nreviewer_groups=[],\nwriters=[NomadUser(name='Joseph Rudzinski')],\nwriter_groups=[],\nviewers=[NomadUser(name='Joseph Rudzinski')],\nviewer_groups=[],\npublished=False,\npublished_to=[],\nwith_embargo=False,\nembargo_length=0.0,\nlicense='CC BY 4.0',\nentries=1,\nn_entries=None,\nupload_files_server_path='/nomad/test/fs/staging/R/RdA_3ZsOTMqbtAhYLivVsw',\npublish_time=None,\nreferences=None,\ndatasets=None,\nexternal_db=None,\nupload_name=None,\ncomment=None,\nurl='https://nomad-lab.eu/prod/v1/test/api/v1',\ncomplete_time=datetime.datetime(2024, 10, 15, 20, 2, 11, 320000))\n</code></pre> <p>One common usage of this function is to ensure that an upload has been processed successfully before making a subsequent action on it, e.g., editing the metadata or publishing. We will apply this functionality in the example below.</p>"},{"location":"api/#deleting-your-upload","title":"Deleting your upload","text":"<p>Before moving on to the example data, let's delete this dummy upload:</p> <pre><code>delete_upload(upload_id, url='test')\n</code></pre> <p>Wait a few seconds to allow for processing and then check to make sure that the upload was deleted:</p> <pre><code>try:\nget_upload_by_id(upload_id, url='test')\nexcept Exception:\nprint(f'Upload with upload_id={upload_id} was deleted successfully.')\n</code></pre> example output <pre><code>'Upload with upload_id=zpq-JTzWQJ63jtSOlbueKA was deleted successfully.'\n</code></pre>"},{"location":"api/#working-with-the-project-data","title":"Working with the project Data","text":"<p>Now that you understand some API basics, we can move to working with the data for the tutorial example project.</p> <p>Create a new notebook <code>Part-2_DFT-calculations.ipynb</code> to work step by step or download the prefilled notebook:</p> <p>Download Part-2_DFT-calculations.ipynb</p> <p>Make all the necessary imports:</p> <pre><code>from dotenv import load_dotenv\nload_dotenv()\nimport os\nimport time\nimport json\nfrom nomad_utility_workflows.utils.entries import download_entry_by_id\nfrom nomad_utility_workflows.utils.uploads import (\nupload_files_to_nomad,\nget_upload_by_id,\nedit_upload_metadata,\npublish_upload\n)\nfrom nomad_utility_workflows.utils.entries import get_entries_of_upload\nfrom nomad_utility_workflows.utils.datasets import create_dataset\n</code></pre> <p>Download the example data from NOMAD and reconstruct the directory structure introduced at the beginning.</p> <pre><code>entries = ['0iCl0nWwCftF0tgQOaklcQLFB68E', '24Q4MoaAUtsWN7Hepw3UH3TU93pX', '6V_q8X39he-dakYHifH_3Z53GTdZ']\nresponses = []\nfor i_entry, entry in enumerate(entries):\nfolder_nm = f'DFT-{i_entry+1}'\nos.makedirs(f'{folder_nm}', exist_ok=True)\nresponses.append(download_entry_by_id(entry, url='prod', zip_file_name=os.path.join(folder_nm, f'{entry}.zip')))\n</code></pre> <p>Note</p> <p>Here we have specified <code>url='prod'</code> since this data is publically available on the NOMAD production deployment. Make sure to use <code>url='test'</code> (or don't specify a url) for other function calls in this tutorial!</p> <p>If the API call was successful you should find a zip file within each <code>DFT-X</code> folder. You can unzip these to ensure that all the raw files are present, i.e., <code>aims.out</code>, <code>aims.in</code>, etc. You can also investigate the <code>response</code> for each API call, which contains the archive (i.e., the full (meta)data stored according to NOMAD's structured schema) for each entry.</p>"},{"location":"api/#iterative-uploading-with-checks","title":"Iterative Uploading with Checks","text":"<p>Although uploads can group multiple entries together, they are limited by the maximum upload size and act more as a practical tool for optimizing the transfer of data to the NOMAD repository. For scientifically relevant connections between entries, NOMAD uses Datasets and Workflows, which will both be covered later in the tutorial.</p> <p>For demonstration purposes, we will upload each of the DFT calculations individually, while implementing some checks to ensure successful processing:</p> <pre><code>entries = ['0iCl0nWwCftF0tgQOaklcQLFB68E', '24Q4MoaAUtsWN7Hepw3UH3TU93pX', '6V_q8X39he-dakYHifH_3Z53GTdZ']\ndft_upload_ids = []\nresponses = []\n# define the timing parameters\nmax_wait_time = 60  # 60 seconds\ninterval = 5  # 5 seconds\nfor i_entry, entry in enumerate(entries):\nfnm = os.path.join(os.getcwd(), f'DFT-{i_entry+1}' ,f'{entry}.zip')\n# make the upload\nupload_id = upload_files_to_nomad(filename=fnm, url='test')\ndft_upload_ids.append(upload_id)\n# wait until the upload is processed successfully before continuing\nelapsed_time = 0\nwhile elapsed_time &lt; max_wait_time:\nnomad_upload = get_upload_by_id(upload_id, url='test')\n# Check if the upload is complete\nif nomad_upload.process_status == 'SUCCESS':\nresponses.append(nomad_upload)\nbreak\n# Wait the specified interval before the next call\ntime.sleep(interval)\nelapsed_time += interval\nelse:\nraise TimeoutError(f'Maximum wait time of {max_wait_time/60.} minutes exceeded. Upload with id {upload_id} is not complete.')\nprint(dft_upload_ids)\n</code></pre> <p>Note</p> <p>It is not necessary to wait between individual uploads, however, it is advisable to check that a given upload process is complete before trying to execute another process on the same upload.</p>"},{"location":"api/#accessing-individual-entries-of-an-upload","title":"Accessing individual entries of an upload","text":"<p>We need to save both the <code>upload_id</code> and <code>entry_id</code> for each DFT upload for use later. As an alternative to copying them manually from the corresponding overview pages, we can get them programmatically:</p> <pre><code>dft_entry_ids = []\nfor upload in dft_upload_ids:\nentries = get_entries_of_upload(upload, url='test', with_authentication=True)\nassert len(entries) == 1\ndft_entry_ids.append(entries[0].entry_id)\nprint(dft_entry_ids)\n</code></pre> <p>This should return a list of 3 entry ids. Copy the list into your <code>PIDs.json</code> file from Part 1:</p> <pre><code>{\n\"upload_ids\": {\n\"md-workflow\": \"&lt;your md workflow upload id from Part 1&gt;\",\n\"DFT\": [\n\"&lt;copy...\",\n\"the list of...\",\n\"dft upload ids&gt;\"\n],\n\"setup-workflow\": \"\",\n\"analysis\": \"\"\n},\n\"entry_ids\": {\n\"md-workflow\": \"&lt;your md workflow entry id from Part 1&gt;\",\n\"DFT\": [\n\"&lt;copy...\",\n\"the list of...\",\n\"dft entry ids&gt;\"\n],\n\"setup-workflow\": \"\",\n\"parameters\": \"\",\n\"analysis\": \"\"\n},\n\"dataset_id\": \"\"\n}\n</code></pre> Additional Exercise <p>If you were really implementing this project managment pipeline, you would of course want to save these IDs automatically without having to copy them into a file. As an additional coding exercise, you could attempt to save the IDs into your file directly from the notebook.</p>"},{"location":"api/#creating-datasets","title":"Creating Datasets","text":"<p>Let's now create a dataset to group all of our uploaded data together, substitute <code>&lt;your_name&gt;</code> before running the command:</p> <pre><code>dataset_id = create_dataset('Example Dataset - DPG Tutorial 2025 - &lt;your_name&gt;', url='test')\nprint(dataset_id)\n</code></pre> <p>Note</p> <p>Make sure to add your name or some unique identifier. Dataset names must be unique.</p> <p>Copy the <code>dataset_id</code> into <code>PIDs.json</code>) for later use. (You can always find it by going to Test Deployment &gt; PUBLISH &gt; Datasets to view all of your created datasets).</p> <pre><code>{\n\"upload_ids\": {\n\"md-workflow\": \"&lt;your md workflow upload id from Part 1&gt;\",\n\"DFT\": [\"&lt;your list of dft upload ids from above&gt;\"],\n\"setup-workflow\": \"\",\n\"analysis\": \"\"\n},\n\"entry_ids\": {\n\"md-workflow\": \"&lt;your md workflow entry id from Part 1&gt;\",\n\"DFT\": [\"&lt;your list of dft entry ids from above&gt;\"],\n\"setup-workflow\": \"\",\n\"parameters\": \"\",\n\"analysis\": \"\"\n},\n\"dataset_id\": \"&lt;copy the dataset id here&gt;\"\n}\n</code></pre>"},{"location":"api/#editing-the-upload-metadata","title":"Editing the upload metadata","text":"<p>Now that your uploads are processed successfully, you can add coauthors, references, and other comments, as well as link to a dataset and provide a name for the upload. Note that the coauthor is specified by an email address that should correspond to the email linked to the person's NOMAD account. See more about searching for users nomad-utility-workflows DOCS &gt; NOMAD User Metadata.</p> <p>The metadata should be stored as a dictionary as follows:</p> <pre><code>metadata = {\n\"metadata\": {\n\"upload_name\": '&lt;new_upload_name&gt;',\n\"references\": [\"https://doi.org/xx.xxxx/xxxxxx\"],\n\"datasets\": '&lt;dataset_id&gt;',\n\"embargo_length\": 0,\n\"coauthors\": [\"coauthor@affiliation.de\"],\n\"comment\": 'This is a test upload...'\n},\n}\n</code></pre> <p>Let's simply add a name to each upload and then link them to the dataset that we created:</p> <pre><code># dft_upload_ids - should be previously defined in your notebook\n# dataset_id - should be previously defined in your notebook\nresponses = []\n# define the timing parameters\nmax_wait_time = 30  # 30 seconds\ninterval = 5  # 5 seconds\nfor i_upload, upload in enumerate(dft_upload_ids):\nmetadata_new = {'upload_name': f'Test Upload - DFT-{i_upload}', 'datasets': dataset_id}\nedit_upload_metadata(upload, url='test', upload_metadata=metadata_new)\n# wait until the upload is processed successfully before continuing\nelapsed_time = 0\nwhile elapsed_time &lt; max_wait_time:\nnomad_upload = get_upload_by_id(upload, url='test')\n# Check if the edit upload is complete\nif nomad_upload.process_status == 'SUCCESS':\nresponses.append(nomad_upload)\nbreak\n# Wait the specified interval before the next call\ntime.sleep(interval)\nelapsed_time += interval\nelse:\nraise TimeoutError(f'Maximum wait time of {max_wait_time/60.} minutes exceeded. Edit Upload with id {upload} is not complete.')\n</code></pre> <p>Let's also add the drag and drop upload with the MD data to our dataset. First, go to Test Deployment &gt; PUBLISH &gt; Uploads and find the <code>upload_id</code> for the MD data.</p> <pre><code>with open(os.path.join('&lt;path to PIDs&gt;', 'PIDs.json')) as f:\npids_dict = json.load(f)\nmd_upload_id = pids_dict.get('upload_ids').get('md')\nmetadata_new = {'upload_name': f'Test Upload - MD-equilibration', 'datasets': dataset_id}\nedit_upload_metadata(md_upload_id, url='test', upload_metadata=metadata_new)\n</code></pre> <p>Before moving on, let's check that this additional process is complete:</p> <pre><code>nomad_upload = get_upload_by_id(md_upload_id, url='test')\nprint(nomad_upload.process_status == 'SUCCESS')\nprint(nomad_upload.process_running is False)\n</code></pre> output <pre><code>True\nTrue\n</code></pre> <p>Now, go to your uploads page in NOMAD to verify that your uploads are now named. Go to <code>PUBLISH &gt; Datasets</code>. You should see the dataset that you created for this tutorial. Click the arrow to the right of this dataset to view all the entries that have been added to your dataset so far:</p>"},{"location":"api/#publishing-uploads","title":"Publishing Uploads","text":"<p>Before publishing uploads, it is advisable to browse the individual entries to make sure there are no obvious problems, e.g., in the visualizations in the <code>Overview</code> tab or in the <code>LOG</code> tab (there is a clear red <code>!</code> to indicate if there were errors in processing). Note that it is possible that the processing reports as successful while still having some errors. If you have too many uploads to inspect manually, choosing a representative subset is advisable. If you find any problems, you can receive assistance by posting on #software-updates on the NOMAD Discord Server\u2014If you are not yet a member of the NOMAD server use Invitation to Discord.</p> <p>We can publish all 4 uploads that we made so far with <code>publish_upload()</code>, making it publicly available on the Test NOMAD deployment (again these will be eventually deleted).</p> <p>Note</p> <p>Once the upload is published you will no longer be able to make changes to the raw files that you uploaded. However, the upload metadata (accessed and edited in the above example) can be changed after publishing.</p> <pre><code>all_upload_ids = [*dft_upload_ids, md_upload_id]\nresponses = []\n# define the timing parameters\nmax_wait_time = 30  # 30 seconds\ninterval = 5  # 5 seconds\nfor upload in all_upload_ids:\npublished_upload = publish_upload(upload, url='test')\n# wait until the upload is processed successfully before continuing\nelapsed_time = 0\nwhile elapsed_time &lt; max_wait_time:\nnomad_upload = get_upload_by_id(upload, url='test')\n# Check if the edit upload is complete\nif nomad_upload.process_status == 'SUCCESS':\nresponses.append(nomad_upload)\nbreak\n# Wait the specified interval before the next call\ntime.sleep(interval)\nelapsed_time += interval\nelse:\nraise TimeoutError(f'Maximum wait time of {max_wait_time/60.} minutes exceeded. Publish Upload with id {upload} is not complete.')\n</code></pre>"},{"location":"api/#more-resources","title":"More Resources","text":"<p>More detailed documentation can be found nomad-utility-workflows DOCS &gt; Perform API Calls. An exhaustive list of API endpoints and commands are explained in the NOMAD API Dashboard.</p>"},{"location":"core/","title":"Part 1: Using NOMAD's Core Functionalities","text":""},{"location":"core/#what-you-will-learn","title":"\ud83c\udfaf What You Will Learn","text":"<ul> <li>How NOMAD processes and structures raw data</li> <li>How to navigate and explore entries via the NOMAD GUI</li> </ul> <p>Approach: You will set up an example project that we\u2019ll use throughout the tutorial\u2014a real-world workflow in which a researcher uploads, links, and publishes a set of heterogeneous data tasks from a scientific study.</p>"},{"location":"core/#example-project","title":"\ud83d\uddc2\ufe0f Example Project","text":"<p>You are a researcher investigating the atomic structure and electronic properties of water. Your project workflow includes multiple simulation stages and analysis steps.</p> <p>The graph below illustrates the structure of your project workflow:</p> <p>Overarching Workflow Tasks:</p> <ol> <li>A series of manual or self-scripted processes for setup.</li> <li>Classical molecular dynamics to generate preliminary structures, using a standard simulation software.</li> <li>Single-point self-consistent-field DFT calculations to determine the electronic properties, again using a standard software.</li> <li>Vibrational analysis using an in-house code.</li> </ol> <p>Challenge: You are preparing a manuscript for publication and have been asked to:</p> <ul> <li>Collect and organize your data</li> <li>Document all methodological steps</li> <li>Ensure reproducibility as fully as possible</li> <li>Make the data publicly accessible upon publication</li> </ul> <p>Your Solution: Use the NOMAD Central Repository to upload, structure, and share your complete project workflow.</p>"},{"location":"core/#the-nomad-repository-and-infrastructure","title":"The NOMAD Repository and Infrastructure","text":"<p>NOMAD is a multifaceted software with a wide range of support for scientific research data focused towards, but not limited to, the materials science community. This tutorial will only cover a very small fraction of NOMAD's functionalities, with the aim to highlight a variey of approaches for documenting data provenance (i.e., the contextual history of data) through the storage of workflow metadata.</p> The NOMAD Ecosystem (numbers as of Feb 2025)"},{"location":"core/#nomad-basics-processing-of-supported-simulation-data","title":"NOMAD Basics - Processing of supported simulation data","text":"<p>NOMAD ingests the raw input and output files from standard simulation software by first identifying a representative file (denoted the mainfile) and then employing a parser code to extract relevant (meta)data from the associated files to that simulation. The (meta)data are stored within a structured schema \u2014the NOMAD Metainfo\u2014to provide context for each quantity, enabling interoperability and comparison between, e.g., simulation software.</p> More Info: Organization in NOMAD <p>Entries: The compilation of all (meta)data obtained from this processing forms an entry\u2500the fundamental unit of storage within the NOMAD database\u2500including simulation input/output, author information, and additional general overarching metadata (e.g., references or comments), as well as an <code>entry_id</code> \u2014 a unique identifier.</p> <p>Uploads: NOMAD entries can be organized hierarchically into uploads. Since the parsing execution is dependent on automated identification of representative files, users are free to arbitrarily group simulations together upon upload. In this case, multiple entries will be created with the corresponding simulation data. An additional unique identifier, <code>upload_id</code>, will be provided for this group of entries. Although the grouping of entries into an upload is not necessarily scientifically meaningful, it is practically useful for submitting batches of files from multiple simulations to NOMAD.</p> <p>Workflows: NOMAD offers flexibility in the construction of workflows. NOMAD also allows the creation of custom workflows, which are completely general directed graphs, allowing users to link NOMAD entries with one another in order to provide the provenance of the simulation data. Custom workflows are contained within their own entries and, thus, have their own set of unique identifiers. To create a custom workflow, the user is required to upload a workflow yaml file describing the inputs and outputs of each entry within the workflow, with respect to sections of the NOMAD Metainfo schema.</p> <p>Datasets: At the highest level, NOMAD groups entries with the use of data sets. A NOMAD data set allows the user to group a large number of entries, without any specification of links between individual entries. A DOI is also generated when a data set is published, providing a convenient route for referencing all data used for a particular investigation within a publication.</p>"},{"location":"core/#drag-and-drop-gui-uploads","title":"Drag and drop GUI uploads","text":"<p>Imagine that you have already performed a standard equilibration workflow for your molecular dynamics simulations, and have organized them in the following directory structure within a zip file:</p> <pre><code>workflow-example-water-atomistic.zip\n\u251c\u2500\u2500 workflow.archive.yaml\n\u251c\u2500\u2500 Emin # Geometry Optimization\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 mdrun_Emin.log # GROMACS mainfile\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ...other raw simulation files\n\u251c\u2500\u2500 Equil-NPT # NPT equilibration\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 mdrun_Equil-NPT.log # GROMACS mainfile\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ...other raw simulation files\n\u2514\u2500\u2500 Prod-NVT # NVT production\n \u00a0\u00a0 \u251c\u2500\u2500 mdrun_Prod-NVT.log # GROMACS mainfile\n \u00a0\u00a0 \u2514\u2500\u2500 ...other raw simulation files\n</code></pre> <p>The simulations were run with the molecular dynamics simulation package GROMACS. As we will see, the <code>.log</code> files will be automatically detected as mainfiles of a GROMACS simulations by NOMAD, followed by the linking to corresponding auxillary files (i.e., other input/output files from that simulation) and, finally, an extraction and storage of all the relevant (metadata) within NOMAD's structured data schema.</p> <p>This example data has been pre-uploaded and published on NOMAD. Go to the example data upload page and download the example files by clicking the  icon. Before you proceed, close the browser window so that you do not mistakenly upload files to the main public deployment of NOMAD.</p> <p>Create a workspace folder for this tutorial, e.g., <code>workspace_nomad_tutorial_workflows/</code>, and then move the downloaded zip to this folder. We suggest also creating sub-folders <code>Part-1</code>-<code>Part-4</code> for organizational purposes.</p> <p>Now go to the Test NOMAD Deployment.</p> <p>Attention</p> <p>All uploads in this tutorial will be sent to the Test Deployment of NOMAD. The data sent there is not persistent, and will be deleted occasionally. Thus, we are free to test all publishing functionalities there. To verify that you are at the correct url, you can check for the word \"test\" in the url string, e.g., https://nomad-lab.eu/prod/v1/test/gui/search/entries.</p> <p>Upload the zip file that you downloaded with the example data as demonstrated in the video below:</p>"},{"location":"core/#browse-the-entry-pages","title":"Browse the entry pages","text":"<p>Click on the right arrows next to each processed entry to browse the overview page of each:</p> <p>Workflow Entry:</p> <p>We will need both the <code>upload_id</code> and the <code>entry_id</code> for this entry later. Copy them from the left-hand <code>MetaData</code> bar, and place them into a file called <code>PIDs.json</code> as follows:</p> <pre><code>{\n\"upload_ids\": {\n\"md-workflow\": \"&lt;enter the copied upload_id here&gt;\",\n\"DFT\": [ \"\", \"\", \"\"],\n\"setup-workflow\": \"\",\n\"analysis\": \"\"\n},\n\"entry_ids\": {\n\"md-workflow\": \"&lt;enter the copied entry_id here&gt;\",\n\"DFT\": [\"\", \"\", \"\"],\n\"setup-workflow\": \"\",\n\"parameters\": \"\",\n\"analysis\": \"\"\n},\n\"dataset_id\": \"\"\n}\n</code></pre> <p>Production Simulation:</p> <p>There a 4 tabs to explore within each entry:</p> <ul> <li> <p>OVERVIEW: a simple description of this entry through visualizations of the system itself, key observables, overarching metadata, workflow graph, and links to other entries (i.e., references).</p> </li> <li> <p>FILES: all the raw data that was uploaded via the .zip file, retained within the original file system structure. These can be previewed and downloaded.</p> </li> <li> <p>DATA: a browser to navigate through the populated NOMAD Metainfo for this entry, i.e., the processed and normalized version of the simulation data and metadata.</p> </li> <li> <p>LOGS: technical information about the data processing along with any warnings or errors that were raised by the NOMAD software.</p> </li> </ul>"},{"location":"custom/","title":"Part 3: Creating Custom Entries in NOMAD","text":""},{"location":"custom/#what-you-will-learn","title":"\ud83c\udfaf What You Will Learn","text":"<ul> <li>How to store data that is not supported by existing NOMAD parsers</li> <li>How to define a custom YAML schema</li> <li>How to use the NOMAD ELN (Electronic Lab Notebook) interface</li> <li>How to connect these custom entries into a workflow graph</li> </ul>"},{"location":"custom/#workflow-tasks-executed-manually-or-via-custom-scripts","title":"\ud83e\uddf0 Workflow Tasks Executed Manually or via Custom Scripts","text":"<p>Challenge: You need to include the simulation setup procedure with sufficient details for reproducibility, but the setup steps were performed either manually or with custom scripts, such that they will not be automatically recognized by NOMAD.</p> <p>Your Solution: Use NOMAD\u2019s ELN and custom schema functionalities!</p> <p>Attention</p> <p>There are some steps in this section that can be performed programmatically or \"manually\" in the GUI. The programmable route is     intended to be performed in a single jupyter session (pre-filled example notebook provided below).</p>"},{"location":"custom/#simulation-setup-steps","title":"Simulation setup steps","text":"<p>Imagine that to set up the MD simulations uploaded in part 1, you had to create structure and topology files. You ran 2 python scripts for this, <code>workflow_script_1.py</code> and <code>workflow_script_2.py</code>. The first script creates the simulation box (stored in <code>box.gro</code>) and inserts the water molecules (stored in <code>water.gro</code>). The second script creates the force field file (<code>water.top</code>).</p> <p>You can download these 5 files and save them for later:</p> <p>Download Simulation Setup Input/Output Files</p> <p>The entire setup workflow can be represented as:</p> <p>This is the exact workflow graph that we aim to generate in NOMAD within this part of the tutorial.</p>"},{"location":"custom/#electronic-lab-notebook-eln-entries-in-nomad","title":"Electronic lab notebook (ELN) entries in NOMAD","text":"<p>Let's explore the basic functionalities of NOMAD ELNs. You can create a basic ELN entry from <code>Your uploads</code> page by clicking <code>CREATE A NEW UPLOAD &gt; CREATE FROM SCHEMA</code> and selecting <code>Basic ELN</code> under the <code>Built-in Schema</code> drop-down menu, as demonstrated in this video:</p> <p>Upon entry creation, you will be taken to the <code>Data</code> tab, where you can fill in or edit the predefined ELN quantities in the user-editable ELN interface. Type a dummy description for this entry and then press the  icon in the upper right. Now, navigate to the <code>Overview</code> page to see your changes there.</p> <p>The editable quantities that you found in your ELN entry (e.g., short name, tags, ID, description) appear because they are defined within the <code>Basic ELN</code> schema that you selected in NOMAD. NOMAD provides a tool for browsing all such schemas. Go to <code>ANALYZE &gt; The NOMAD MetaInfo</code>, then select <code>nomad &gt; Basic ELN</code> to view all the quantity definitions and descriptions within this entry class:</p>"},{"location":"custom/#creating-an-eln-entry-from-yaml","title":"Creating an ELN entry from YAML","text":"<p>Analogous to the simulation code parsers, NOMAD has a parser for its native schema \u2014 the NOMAD MetaInfo. This parser is automatically executed for files named <code>&lt;file_name&gt;.archive.yaml</code>. In this way, users can create ELN entries by uploading a yaml file populated according to NOMAD's schema.</p> <p>For example, we can create a basic ELN entry by creating and uploading a file, e.g. <code>basic_eln_entry.archive.yaml</code>, with the contents:</p> <pre><code>data:\nm_def: \"nomad.datamodel.metainfo.eln.ElnBaseSection\"\nname: \"ELN entry from YAML\"\ndescription: \"A test ELN entry...\"\n</code></pre> <p>The <code>data</code> section is created and defined as type <code>ElnBaseSection</code>, meaning that we can populate all the quantities (e.g., name and description) living in this section (as seen in the MetaInfo browser above).</p> <p>Uploading this yaml to the test deployment results in an entry with the overview page:</p>"},{"location":"custom/#customizing-the-schema","title":"Customizing the schema","text":"<p>To document our simulation setup workflow, we need to reference files within our ELN entry. For standardization and search capabilities, it is best practice to use existing classes in the MetaInfo. However, NOMAD also allows users to customize the schema to their own specific needs. Let's create our own schema to store annotated files within an ELN.</p> <p>Attention</p> <p>Understanding the details of the customized schema below is beyond the scope of this tutorial, and is not necessarily the adviced route for the most robust customization. The important take away is that you can in principal create you own schema. In this case we have done so to enable storage of annotated files.</p> <p>Create a file <code>ELNFiles.archive.yaml</code> with the following contents:</p> <code>ELNFiles.archive.yaml</code> <pre><code>definitions: # Use the defintions section to create your schema\nname: 'ELN-Annotated-File-List'\nsections:\nAnnotatedFile: # A subsection for storing an annotated file\nm_annotations:\neln:\noverview: True # Displays this quantity in the overview page of the entry\nquantities:\nfile: # a quantity for storing the actualy file reference\ntype: str\ndescription: single workflow files\nm_annotations:\nbrowser:\nadaptor: RawFileAdaptor  # Allows to navigate to files in the data browser\neln:\ncomponent: FileEditQuantity # Allows editing with the GUI\ndescription: # a quantity for storing the annotation\ntype: str\ndescription: describe the file\nm_annotations:\neln:\ncomponent: StringEditQuantity # Allows editing within the GUI\nELNAnnotatedFiles: # Define a subsection for storing files\nbase_sections:\n- 'nomad.datamodel.metainfo.eln.ElnBaseSection' # inherits from the basic ELN class\n- 'nomad.datamodel.data.EntryData' # necessary when a class will be the root of our archive\nm_annotations:\neln:\nhide: ['lab_id'] # hides the lab_id quantity that we will not use\nsub_sections:\nFiles:\nrepeats: True # makes the subsection repeating (i.e., a list)\nsection: '#/AnnotatedFile' # this subsection will include the quantities defined within the `AnnotedFile` class defined above\n</code></pre> <p>The section <code>AnnotationFile</code> contains 2 quantities <code>file</code> and <code>description</code> for storing a file reference and annotation, respectively. The section <code>ELNAnnotatedFiles</code> extends the most basic ELN implementation (<code>ElnBaseSection</code>) with a repeating subsection of type <code>AnnotatedFile</code>. In this way, our ELN will be able to store a list of annotated files.</p> More on custom schemas <p>The YAML approach is a quick and dirty way to customize your NOMAD entries. To disincentivise the proliferation of ad-hoc schemas and remain FAIR, YAML sections or quantities only have partial support. Seamless integration of new quantities happens when schemas are organized in plugins (python packages) and installed in NOMAD during deployment. See NOMAD Docs &gt; How to write a YAML schema package for more details about defining custom schemas in this way.</p> <p>The more robust and powerful approach for creating custom schemas is to create a schema plugin (see NOMAD Docs &gt; How to get started with plugins).</p> <p>Useful resources for plugin developers are the Plugin Template and the NOMAD Distro Template.</p> <p>We can now use these definitions to create an entry file for the step of creating the force field file (as illustrated in the image above):</p> <code>create_force_field.archive.yaml</code> <pre><code>data:\nm_def: '../upload/raw/Custom_ELN_Entries/ELNFiles.archive.yaml#ELNAnnotatedFiles'\nname: 'Create force field'\ndescription: 'The force field is defined for input to the MD simulation engine.'\nFiles:\n- file: 'Custom_ELN_Entries/water.top'\ndescription: 'The force field file for simulation input.'\n</code></pre> <p>Here we define the data section using our <code>ELNFiles.archive.yaml</code> schema. The given path is a relative path assuming that we will upload these 2 files (i.e., <code>ELNFiles.archive.yaml</code> and <code>create_force_field.archive.yaml</code>) within the same upload with a root folder called <code>Custom_ELN_Entries</code>.</p> <p>You can now create analogous files <code>create_box.archive.yaml</code>, <code>insert_water.archive.yaml</code>, <code>workflow_parameters.archive.yaml</code>, <code>workflow_scripts.archive.yaml</code>:</p> <code>create_box.archive.yaml</code> <pre><code>data:\nm_def: '../upload/raw/Custom_ELN_Entries/ELNFiles.archive.yaml#ELNAnnotatedFiles'\nname: 'Create box'\ndescription: 'The initial simulation box is created.'\nFiles:\n- file: 'Custom_ELN_Entries/box.gro'\ndescription: 'An empty structure file with the box vectors.'\n</code></pre> <code>insert_water.archive.yaml</code> <pre><code>data:\nm_def: '../upload/raw/Custom_ELN_Entries/ELNFiles.archive.yaml#ELNAnnotatedFiles'\nname: 'Insert water'\ndescription: 'Water is inserted into the simulation box, creating the structure file for simulation input.'\nFiles:\n- file: 'Custom_ELN_Entries/water.gro'\ndescription: 'The structure file for simulation input.'\n</code></pre> <code>workflow_parameters.archive.yaml</code> <pre><code>data:\nm_def: nomad.datamodel.metainfo.eln.ElnBaseSection\nname: 'Workflow Parameters'\ndescription: 'This is a description of the overall workflow parameters, or alternatively standard workflow specification...'\n</code></pre> <code>workflow_scripts.archive.yaml</code> <pre><code>data:\nm_def: '../upload/raw/Custom_ELN_Entries/ELNFiles.archive.yaml#ELNAnnotatedFiles'\nname: 'Workflow Scripts'\ndescription: 'All the scripts run during setup of the MD simulation.'\nFiles:\n- file: 'Custom_ELN_Entries/workflow_script_1.py'\ndescription: 'Creates the simulation box and inserts water molecules.'\n- file: 'Custom_ELN_Entries/workflow_script_2.py'\ndescription: 'Creates the appropriate force field files for the simulation engine.'\n</code></pre>"},{"location":"custom/#creating-a-custom-workflow-in-nomad","title":"Creating a custom workflow in NOMAD","text":"<p>NOMAD allows users to connect entries into a workflow, i.e., a directed graph structure. This is achieved using the same parsing functionality as demonstrated with the custom schemas above. In this case, we simply populate the <code>workflow2</code> section instead of the <code>data</code> section. When uploaded to NOMAD, a new workflow entry will be created, with references to each of the workflow tasks, and also an interactive workflow graph for easy navigation of the entire workflow. Learn more about the archive file structure in the official NOMAD documentation.</p> <p>Let's construct this workflow yaml piece by piece, starting with the section definition and global inputs/outputs:</p> <pre><code>\"workflow2\":\n\"name\": \"MD Setup workflow\"\n\"inputs\":\n- \"name\": \"workflow parameters\"\n\"section\": \"&lt;path_to_mainfile&gt;/workflow_parameters.archive.yaml#/data\"\n- \"name\": \"workflow scripts\"\n\"section\": \"&lt;path_to_mainfile&gt;/workflow_scripts.archive.yaml#/data/Files\"\n\"outputs\":\n- \"name\": \"structure file\"\n\"section\": \"&lt;path_to_mainfile&gt;/insert_water.archive.yaml#/data/Files/0/file\"\n- \"name\": \"force field file\"\n\"section\": \"&lt;path_to_mainfile&gt;/create_force_field.archive.yaml#/data/Files/0/file\"\n</code></pre> <p>This example denotes full path to each yaml file with placeholders like <code>&lt;path_to_mainfile&gt; = ../upload/archive/mainfile/Custom_ELN_Entries/</code>. As we already saw above, the <code>../upload/</code> syntax is used to access files that were uploaded together. The <code>archive/mainfile</code> directory can be used to access all the mainfiles (i.e., files automatically recognized by NOMAD). <code>Custom_ELN_Entries/</code> is the user-defined folder in which the upload is contained.</p> <p>This workflow takes as input the entire \"workflow parameters\" entry and a list of workflow scripts, and outputs the structure and force field files.</p> <p>We now need to define each task, which contains its own inputs and outputs, e.g., the task that creates the force field file:</p> <pre><code>\"workflow2\":\n... ### I/Os\n\"tasks\":\n... ### Other tasks\n- \"m_def\": \"nomad.datamodel.metainfo.workflow.TaskReference\"\n\"name\": \"create force field\"\n\"task\": \"&lt;path_to_mainfile&gt;/create_force_field.archive.yaml#/data\"\n\"inputs\":\n- \"name\": \"workflow parameters\"\n\"section\": \"&lt;path_to_mainfile&gt;/workflow_parameters.archive.yaml#/data\"\n- \"name\": \"workflow script 2\"\n\"section\": \"&lt;path_to_mainfile&gt;/workflow_scripts.archive.yaml#/data/Files/1/file\"\n\"outputs\":\n- \"name\": \"force field file\"\n\"section\": \"&lt;path_to_mainfile&gt;/create_force_field.archive.yaml#/data/Files/0/file\"\n</code></pre> <p>This task is linked to the entry defined in <code>create_force_field.archive.yaml</code>. It takes as input: 1. the entire workflow parameters entry, defined in <code>workflow_parameters.archive.yaml</code>, and 2. The second file stored in the files list within the workflow scripts entry, defined by <code>workflow_scripts.archive.yaml</code>. The output of this task is the force field file, which is the first file stored in the file list of the create for field entry.</p> <p>You can now add the \"create box\" and \"insert water\" tasks to create the final workflow file:</p> <code>setup_workflow.archive.yaml</code> <pre><code>\"workflow2\":\n\"name\": \"MD Setup workflow\"\n\"inputs\":\n- \"name\": \"workflow parameters\"\n\"section\": \"&lt;path_to_mainfile&gt;/workflow_parameters.archive.yaml#/data\"\n- \"name\": \"workflow scripts\"\n\"section\": \"&lt;path_to_mainfile&gt;/workflow_scripts.archive.yaml#/data/Files\"\n\"outputs\":\n- \"name\": \"structure file\"\n\"section\": \"&lt;path_to_mainfile&gt;/insert_water.archive.yaml#/data/Files/0/file\"\n- \"name\": \"force field file\"\n\"section\": \"&lt;path_to_mainfile&gt;/create_force_field.archive.yaml#/data/Files/0/file\"\n\"tasks\":\n- \"m_def\": \"nomad.datamodel.metainfo.workflow.TaskReference\"\n\"name\": \"create box\"\n\"task\": \"&lt;path_to_mainfile&gt;/create_box.archive.yaml#/data\"\n\"inputs\":\n- \"name\": \"workflow parameters\"\n\"section\": \"&lt;path_to_mainfile&gt;/workflow_parameters.archive.yaml#/data\"\n- \"name\": \"workflow script 1\"\n\"section\": \"&lt;path_to_mainfile&gt;/workflow_scripts.archive.yaml#/data/Files/0/file\"\n\"outputs\":\n- \"name\": \"initial box\"\n\"section\": \"&lt;path_to_mainfile&gt;/create_box.archive.yaml#/data/Files/0/file\"\n- \"m_def\": \"nomad.datamodel.metainfo.workflow.TaskReference\"\n\"name\": \"insert water\"\n\"task\": \"&lt;path_to_mainfile&gt;/insert_water.archive.yaml#/data\"\n\"inputs\":\n- \"name\": \"initial box\"\n\"section\": \"&lt;path_to_mainfile&gt;/create_box.archive.yaml#/data/Files/0/file\"\n- \"name\": \"workflow script 1\"\n\"section\": \"&lt;path_to_mainfile&gt;/workflow_scripts.archive.yaml#/data/Files/0/file\"\n\"outputs\":\n- \"name\": \"structure file\"\n\"section\": \"&lt;path_to_mainfile&gt;/insert_water.archive.yaml#/data/Files/0/file\"\n- \"m_def\": \"nomad.datamodel.metainfo.workflow.TaskReference\"\n\"name\": \"create force field\"\n\"task\": \"&lt;path_to_mainfile&gt;/create_force_field.archive.yaml#/data\"\n\"inputs\":\n- \"name\": \"workflow parameters\"\n\"section\": \"&lt;path_to_mainfile&gt;/workflow_parameters.archive.yaml#/data\"\n- \"name\": \"workflow script 2\"\n\"section\": \"&lt;path_to_mainfile&gt;/workflow_scripts.archive.yaml#/data/Files/1/file\"\n\"outputs\":\n- \"name\": \"force field file\"\n\"section\": \"&lt;path_to_mainfile&gt;/create_force_field.archive.yaml#/data/Files/0/file\"\n</code></pre> <p>Create a new folder called <code>Custom_ELN_Entries</code> and place in it all of the completed files. Don't forget to:</p> <ul> <li>replace <code>&lt;path_to_mainfile&gt;</code> with <code>../upload/archive/mainfile/Custom_ELN_Entries/</code> in the last created file <code>setup_workflow.archive.yaml</code></li> <li>include the 5 files previously downloaded (<code>workflow_script_1.py</code>, <code>workflow_script_2.py</code>, <code>box.gro</code>, <code>water.gro</code>, <code>water.top</code>).</li> </ul> <p>Alternatively, you can download the complete folder here:</p> <p>Download Custom_ELN_Entries folder</p>"},{"location":"custom/#uploading-and-publishing","title":"Uploading and publishing","text":"<p>We now need to upload these files, edit the metadata, and publish the upload. You have the choice to use either the GUI, as demonstrated in Part 1, or the API, as demonstrated in Part 2.</p>"},{"location":"custom/#using-the-gui","title":"Using the GUI","text":"<p>If you use the GUI upload, you will need to find and use the <code>Edit Metadata</code> button on the uploads page in order to add the <code>dataset_id</code> manually to link the upload with your dataset.</p> <p>Once you have published the upload, continue with Saving the PIDs below.</p>"},{"location":"custom/#using-the-api","title":"Using the API","text":"<p>Create a new notebook <code>Custom_ELN_Entries.ipynb</code> to try the steps below on your own or download the prefilled notebook:</p> <p>Download Custom_ELN_Entries.ipynb</p> <ul> <li>Make the imports:</li> </ul> Solution <pre><code>import os\nimport time\nfrom nomad_utility_workflows.utils.uploads import (\nupload_files_to_nomad,\nget_upload_by_id,\nedit_upload_metadata,\npublish_upload\n)\n</code></pre> <ul> <li>Zip the folder for upload:</li> </ul> Solution <pre><code>os.system('zip -r Custom_ELN_Entries.zip Custom_ELN_Entries/')\n</code></pre> <ul> <li>Upload to NOMAD and check for completion of processing:</li> </ul> Solution <pre><code>fnm = 'Custom_ELN_Entries.zip'\n# define the timing parameters\nmax_wait_time = 60  # 60 seconds\ninterval = 5  # 5 seconds\n# make the upload\neln_entries_upload_id = upload_files_to_nomad(filename=fnm, url='test')\n# wait until the upload is processed successfully before continuing\nelapsed_time = 0\nwhile elapsed_time &lt; max_wait_time:\nnomad_upload = get_upload_by_id(eln_entries_upload_id, url='test')\n# Check if the upload is complete\nif nomad_upload.process_status == 'SUCCESS':\nbreak\n# Wait the specified interval before the next call\ntime.sleep(interval)\nelapsed_time += interval\nelse:\nraise TimeoutError(f'Maximum wait time of {max_wait_time/60.} minutes exceeded. Upload with id {eln_entries_upload_id} is not complete.')\nprint(eln_entries_upload_id)\n</code></pre> <ul> <li>Add a title and link to your dataset:</li> </ul> Solution <pre><code>dataset_id = '&lt;your dataset id&gt;'\nmetadata_new = {'upload_name': f'Test Upload - ELN Entries', 'datasets': dataset_id}\n_ = edit_upload_metadata(eln_entries_upload_id, url='test', upload_metadata=metadata_new)\ntime.sleep(10)\nnomad_upload = get_upload_by_id(eln_entries_upload_id, url='test')\nprint(nomad_upload.process_status == 'SUCCESS')\nprint(nomad_upload.process_running is False)\n</code></pre> <ul> <li>Go to NOMAD and inspect your upload. If everything looks correct, go ahead and publish the upload:</li> </ul> Solution <pre><code># define the timing parameters\nmax_wait_time = 30  # 30 seconds\ninterval = 5  # 5 seconds\npublished_upload = publish_upload(eln_entries_upload_id, url='test')\n# wait until the upload is processed successfully before continuing\nelapsed_time = 0\nwhile elapsed_time &lt; max_wait_time:\nnomad_upload = get_upload_by_id(eln_entries_upload_id, url='test')\n# Check if the edit upload is complete\nif nomad_upload.process_status == 'SUCCESS':\nbreak\n# Wait the specified interval before the next call\ntime.sleep(interval)\nelapsed_time += interval\nelse:\nraise TimeoutError(f'Maximum wait time of {max_wait_time/60.} minutes exceeded. Publish Upload with id {eln_entries_upload_id} is not complete.')\n</code></pre>"},{"location":"custom/#saving-the-pids","title":"Saving the PIDs","text":"<p>For Part 4, we will need the entry ids for the setup workflow entry (<code>setup_workflow.archive.yaml</code>) and the workflow parameters entry (<code>workflow_parameters.archive.yaml</code>), that is the input for the md setup workflow. Find the proper entry ids using the GUI or the <code>get_entries_of_upload()</code> method as in Part 2. Copy the <code>entry_id</code> for each into your <code>PIDs.json</code> file:</p> <pre><code>{\n\"upload_ids\": {\n\"md-workflow\": \"&lt;your md workflow upload id from Part 1&gt;\",\n\"DFT\": [\"&lt;your list of dft upload ids from above&gt;\"],\n\"setup-workflow\": \"&lt;copy the setup workflow upload id here&gt;\",\n\"analysis\": \"\"\n},\n\"entry_ids\": {\n\"md-workflow\": \"&lt;your md workflow entry id from Part 1&gt;\",\n\"DFT\": [\"&lt;your list of dft entry ids from above&gt;\"],\n\"setup-workflow\": \"&lt;copy the setup workflow entry id here&gt;\",\n\"parameters\": \"&lt;copy the workflow parameters entry id here&gt;\",\n\"analysis\": \"\"\n},\n\"dataset_id\": \"&lt;your dataset id&gt;\"\n}\n</code></pre>"},{"location":"workflows/","title":"Part 4: Creating Custom Workflows to Link Multiple Uploads","text":""},{"location":"workflows/#what-you-will-learn","title":"\ud83c\udfaf What You Will Learn","text":"<ul> <li>How to define and create a custom workflow entry in NOMAD</li> <li>How to use the <code>nomad-utility-workflows</code> module to generate a workflow YAML</li> <li>How to connect individual uploads as linked tasks in a workflow graph</li> <li>How to add custom plots to your project overview</li> <li>How to obtain a DOI for your complete workflow</li> </ul> <p>You\u2019ll finalize the example project by creating a structured, interactive representation of the entire research process\u2014from input data through all processing steps to final analysis.</p>"},{"location":"workflows/#custom-plots-and-hierarchical-workflows","title":"\ud83e\udde9 Custom Plots and Hierarchical Workflows","text":"<p>Workflows are an important aspect of data as they explain how the data was generated and are essential for reproducibility. In this sense, a workflow has already happened and has produced input and output data that are linked through tasks that have been performed. This often is also referred to as data provenance or provenance graph.</p> <p>Challenge:</p> <ol> <li>You now need to create an overarching workflow that documents the steps executed in your project.</li> <li>You also want to showcase some of the analysis plots that you created from the data generated by the workflow.</li> </ol> <p>Your Solution:</p> <ol> <li>Use NOMAD's plotly functionalities to create entries with custom plots on the overview page.</li> <li>Use <code>nomad-utility-workflows</code> to generate a workflow YAML for the overarching workflow.</li> </ol>"},{"location":"workflows/#plotting-entry","title":"Plotting Entry","text":"<p>You have performed some vibrational analysis of the DFT configurations obtained from your simulations. The results are shown below. Create a file <code>result-vibrational-analysis-DFT.csv</code> to store the results:</p> <pre><code>electron_density,oh_stretch_frequency\ne/A^3,1/cm\n0.0102,3601.\n0.0125,3554.\n0.0140,3507.\n0.0158,3473.\n0.0169,3448.\n0.0184,3392.\n0.0195,3355.\n0.0208,3321.\n0.0223,3289.\n0.0236,3267.\n0.0248,3230.\n0.0261,3194.\n</code></pre> <p>NOMAD has a variety of tools, including plotting functionalities, that can be utilized when defining a custom schema. Let's create an ELN entry that will plot the results of the vibrational analysis.</p> <p>Attention</p> <p>Understanding the details of the customized plotting schema below is beyond the scope of this tutorial, and is not necessarily the adviced route for the most robust plotting customization. The important take away is that you can in principal create you own plotting schema. In this case we have done so to create a very basic plot within an entry.</p> <p>Create a file <code>vibrational_plot_schema.archive.yaml</code> with the following content:</p> <pre><code>\"definitions\":\n\"name\": This is a parser for vibrational analysis data in the .csv format\n\"sections\":\n\"Vibrational_Analysis\":\n\"base_sections\":\n- nomad.datamodel.data.EntryData\n- nomad.parsing.tabular.TableData\n- nomad.datamodel.metainfo.plot.PlotSection\n\"quantities\":\n\"data_file\":\n\"type\": str\n\"descritpion\": Upload your .csv data file\n\"m_annotations\":\n\"eln\":\n\"component\": FileEditQuantity\n\"browser\":\n\"adaptor\": RawFileAdaptor\n\"tabular_parser\":\n\"parsing_options\":\n\"comment\": \"#\"\n\"skiprows\": [1]\n\"mapping_options\":\n- \"mapping_mode\": column\n\"file_mode\": current_entry\n\"sections\":\n- \"#root\"\n\"Electron_Density\":\n\"type\": np.float64\n\"shape\": [\"*\"]\n\"m_annotations\":\n\"tabular\":\n\"name\": electron_density\n\"OH_Stretch_Frequency\":\n\"type\": np.float64\n\"shape\": [\"*\"]\n\"m_annotations\":\n\"tabular\":\n\"name\": oh_stretch_frequency\n\"m_annotations\":\n\"plotly_graph_object\":\n\"data\":\n\"x\": \"#Electron_Density\"\n\"y\": \"#OH_Stretch_Frequency\"\n\"layout\":\n\"title\": Vibrational Analysis\n</code></pre> <p>Here we will not describe in detail the plotting annotations. Rather, this serves as a simple demonstration that custom plotting is possible. In practice, there are various routes for creating custom visualizations. See NOMAD Docs &gt; Reference &gt; Annotations for more information.</p> <p>To create an entry according to this schema, create the file <code>vibrational_analysis.archive.yaml</code> with the following contents:</p> <pre><code>\"data\":\n\"m_def\": \"../upload/raw/vibrational_plot_schema.archive.yaml#Vibrational_Analysis\"\n\"data_file\": \"result-vibrational-analysis-DFT.csv\"\n</code></pre> <p>Alternatively, you can download all 3 files here:</p> <p>Download Vibrational Analysis Files</p> <p>Now we can once again use either the GUI or the API to upload, edit metadata (add a title and link to the dataset), and publish. We will not repeat the steps here, but encourage you to try to repeat the API steps on your own. You can also download the prefilled <code>Vibrational_Analysis.ipynb</code> to perform these steps:</p> <p>Download Vibrational Analysis Upload and Publish Notebook</p> <p>After you have completed the publishing, save the <code>entry_id</code> to your <code>PIDs.json</code>, as we will need it in the final section of the tutorial below:</p> <pre><code>{\n\"upload_ids\": {\n\"md-workflow\": \"&lt;your md workflow upload id from Part 1&gt;\"\n},\n\"entry_ids\": {\n\"md-workflow\": \"&lt;your md workflow entry id from Part 1&gt;\",\n\"DFT\": [\"&lt;your list of dft entry ids from above&gt;\"],\n\"setup-workflow\": \"&lt;your setup workflow entry id from Part 3&gt;\",\n\"parameters\": \"&lt;your workflow parameters entry id from Part 3&gt;\",\n\"analysis\": \"&lt;copy the vibrational analysis entry id here&gt;\"\n},\n\"dataset_id\": \"&lt;your dataset id&gt;\"\n}\n</code></pre> <pre><code>{\n\"upload_ids\": {\n\"md-workflow\": \"&lt;your md workflow upload id from Part 1&gt;\",\n\"DFT\": [\"&lt;your list of dft upload ids from above&gt;\"],\n\"setup-workflow\": \"&lt;your setup workflow upload id here&gt;\",\n\"analysis\": \"&lt;copy the vibrational analysis upload id here&gt;\"\n},\n\"entry_ids\": {\n\"md-workflow\": \"&lt;your md workflow entry id from Part 1&gt;\",\n\"DFT\": [\"&lt;your list of dft entry ids from above&gt;\"],\n\"setup-workflow\": \"&lt;your setup workflow entry id here&gt;\",\n\"parameters\": \"&lt;your workflow parameters entry id here&gt;\",\n\"analysis\": \"&lt;copy the vibrational analysis entry id here&gt;\"\n},\n\"dataset_id\": \"&lt;your dataset id&gt;\"\n}\n</code></pre>"},{"location":"workflows/#creating-the-overarching-project-workflow","title":"Creating the overarching project workflow","text":"<p>Now that all the individual tasks and sub-workflows for the project are stored in the NOMAD repository, we need to create an overarching workflow to connect these components. The final workflow graph should look as follows:</p> <p>As in Part 3, we could create the necessary <code>archive.yaml</code> manually. However, there are many cases where this can be quite tedious and require detailed knowledge of the NOMAD schema. For that reason <code>nomad-utility-workflows</code> includes some tools for automating the generation of this file. In the following, we will demonstrate the basic functionalities of these tools. For more details see <code>nomad-utility-workflow</code> Docs.</p>"},{"location":"workflows/#creating-a-graph-representation-of-the-project-workflow","title":"Creating a graph representation of the project workflow","text":"<p>Create a new notebook <code>Generate_Workflow_Yaml.ipynb</code> to work step by step or download the prefilled notebook:</p> <p>Download Generate_Workflow_Yaml.ipynb</p> <p>Make the imports:</p> <pre><code>import json\nimport gravis as gv\nimport networkx as nx\nfrom nomad_utility_workflows.utils.workflows import (\nNodeAttributes,\nNodeAttributesUniverse,\nbuild_nomad_workflow,\nnodes_to_graph,\n)\n</code></pre> <p>Load the saved PIDs:</p> <pre><code>with open('PIDs.json') as f:\npids_dict = json.load(f)\nentry_ids = pids_dict.get('entry_ids')\nupload_ids = pids_dict.get('upload_ids')\n</code></pre> <p>Create a dictionary with inputs, outputs, and tasks as follows:</p> <pre><code>node_attributes = {\n0: NodeAttributes(\nname='Workflow Parameters',\ntype='input',\npath_info={\n'entry_id': entry_ids.get('parameters'),\n'upload_id': upload_ids.get('setup-workflow'),\n'mainfile_path': 'workflow_parameters.archive.yaml',\n'archive_path': 'data',\n},\n),\n1: NodeAttributes(\nname='MD Setup',\ntype='workflow',\npath_info={\n'entry_id': entry_ids.get('setup-workflow'),\n'upload_id': upload_ids.get('setup-workflow'),\n'mainfile_path': 'setup_workflow.archive.yaml',\n},\nin_edge_nodes=[0],\n),\n2: NodeAttributes(\nname='MD Equilibration',\ntype='workflow',\npath_info={\n'entry_id': entry_ids.get('md-workflow'),\n'upload_id': upload_ids.get('md-workflow'),\n'mainfile_path': 'workflow.archive.yaml',\n},\nin_edge_nodes=[1],\n),\n3: NodeAttributes(\nname='DFT-1',\ntype='workflow',\nentry_type='simulation',\npath_info={\n'entry_id': entry_ids.get('DFT')[0],\n'upload_id': upload_ids.get('DFT')[0],\n'mainfile_path': 'aims.out'\n},\nin_edge_nodes=[2],\nout_edge_nodes=[6],\n),\n4: NodeAttributes(\nname='DFT-2',\ntype='task',\nentry_type='simulation',\npath_info={\n'entry_id': entry_ids.get('DFT')[1],\n'upload_id': upload_ids.get('DFT')[1],\n'mainfile_path': 'aims.out'\n},\nin_edge_nodes=[2],\nout_edge_nodes=[6],\n),\n5: NodeAttributes(\nname='DFT-3',\ntype='task',\nentry_type='simulation',\npath_info={\n'entry_id': entry_ids.get('DFT')[2],\n'upload_id': upload_ids.get('DFT')[2],\n'mainfile_path': 'aims.out'\n},\nin_edge_nodes=[2],\nout_edge_nodes=[6],\n),\n6: NodeAttributes(\nname='Vibrational Analysis',\ntype='output',\npath_info={\n'entry_id': entry_ids.get('analysis'),\n'upload_id': upload_ids.get('analysis'),\n'mainfile_path': 'vibrational_analysis.archive.yaml',\n'archive_path': 'data',\n},\n),\n}\n</code></pre> <p>Validate your dictionary by creating a <code>NodeAttributesUniverse</code> object:</p> <pre><code>node_attributes_universe = NodeAttributesUniverse(nodes=node_attributes)\n</code></pre> <p>This dictionary of node attributes directly informs the creation of the workflow YAML without explicitly referencing specific sections of the NOMAD schema. Specification of the mainfile recognized by the NOMAD parsers is required. In this case, we also include the appropriate entry ids to link together the entries that we have already created. Detailed information about all possible attributes can be found in <code>nomad-utility-workflows</code> Docs &gt; Explanation &gt; Workflows</p> <p>Now, to create a graph of your workflow simply run:</p> <pre><code>workflow_graph_input = nodes_to_graph(node_attributes_universe)\n</code></pre> <p>The result can be visualized with:</p> <pre><code>gv.d3(\nworkflow_graph_input,\nnode_label_data_source='name',\nedge_label_data_source='name',\nzoom_factor=1.5,\nnode_hover_tooltip=True,\n)\n</code></pre>"},{"location":"workflows/#generate-the-workflow-yaml","title":"Generate the workflow yaml","text":"<p>First designate the full path and filename for your YAML and a name for your workflow:</p> <pre><code>workflow_metadata = {\n'destination_filename': './project_workflow.archive.yaml',\n'workflow_name': 'NOMAD Tutorial Workflows Project',\n}\n</code></pre> <p>The workflow name will show up on top of the workflow graph visualization on the overview page of the workflow entry that we will create.</p> <p>We can now use the generated graph to create the workflow YAML using the <code>build_nomad_workflow()</code> function:</p> <pre><code>workflow_graph_output = build_nomad_workflow(\n    workflow_metadata=workflow_metadata,\n    workflow_graph=nx.DiGraph(workflow_graph_input),\n    write_to_yaml=True,\n)\n</code></pre> <p>Another workflow graph is returned by this function:</p> <pre><code>gv.d3(\nworkflow_graph_output,\nnode_label_data_source='name',\nedge_label_data_source='name',\nzoom_factor=1.5,\nnode_hover_tooltip=True,\n)\n</code></pre> <p>We see that our output graph looks signficantly different than the input. That's because <code>nomad-utility-workflow</code> is automatically adding some default inputs/outputs to ensure the proper node connections within the workflow visualizer. For nodes without an <code>entry_type</code>, these default connections work by simply adding inputs/outputs that point to the mainfile of one of the nodes connected by an edge in the graph.</p> example <code>project_workflow.archive.yaml</code> <pre><code>'workflow2':\n'name': 'NOMAD Tutorial Workflows Project'\n'inputs':\n- 'name': 'Workflow Parameters'\n'section': '../uploads/ME2oYBdiQUW4CGcG0YsGuw/archive/ujuIHCdj7StVCxbvYP7NjMvG4v22#/data'\n'outputs':\n- 'name': 'Vibrational Analysis'\n'section': '../uploads/yyqHpBFOSxqhrNALIJswSw/archive/zv4Q_jnvhsGry4oEyj5AC2qD103H#/data'\n- 'name': 'output system from DFT-1'\n'section': '../uploads/JHsC4UFnRtCB9dQYR1BO6A/archive/d2ZJkTjL4LoxdFAVS8YT_jlZdzhm#/run/0/system/-1'\n- 'name': 'output calculation from DFT-1'\n'section': '../uploads/JHsC4UFnRtCB9dQYR1BO6A/archive/d2ZJkTjL4LoxdFAVS8YT_jlZdzhm#/run/0/calculation/-1'\n- 'name': 'output system from DFT-2'\n'section': '../uploads/exkqL5J8Q62ldMHbuE5tcQ/archive/zeiiGOoZL0dikRgYSvzuZZ3xPv9m#/run/0/system/-1'\n- 'name': 'output calculation from DFT-2'\n'section': '../uploads/exkqL5J8Q62ldMHbuE5tcQ/archive/zeiiGOoZL0dikRgYSvzuZZ3xPv9m#/run/0/calculation/-1'\n- 'name': 'output system from DFT-3'\n'section': '../uploads/0wUg7_GuSLiG3U1LcNlr2A/archive/8x9j0tEtx6LeGVEv7ByWD3k5VK1F#/run/0/system/-1'\n- 'name': 'output calculation from DFT-3'\n'section': '../uploads/0wUg7_GuSLiG3U1LcNlr2A/archive/8x9j0tEtx6LeGVEv7ByWD3k5VK1F#/run/0/calculation/-1'\n'tasks':\n- 'm_def': 'nomad.datamodel.metainfo.workflow.TaskReference'\n'name': 'MD Setup'\n'task': '../uploads/ME2oYBdiQUW4CGcG0YsGuw/archive/8EcTSRbvb8PiYaG_TwqAgOYiE8J7#/workflow2'\n'inputs':\n- 'name': 'input data from Workflow Parameters'\n'section': '../uploads/ME2oYBdiQUW4CGcG0YsGuw/archive/ujuIHCdj7StVCxbvYP7NjMvG4v22#/data'\n'outputs':\n- 'name': 'output workflow2 from MD Setup'\n'section': '../uploads/ME2oYBdiQUW4CGcG0YsGuw/archive/8EcTSRbvb8PiYaG_TwqAgOYiE8J7#/workflow2'\n- 'm_def': 'nomad.datamodel.metainfo.workflow.TaskReference'\n'name': 'MD Equilibration'\n'task': '../uploads/7Ncu4YyXTTyTSfg0qpiziQ/archive/J3Vkhz2NAtSw4-KTnbHqUyPhhY3g#/workflow2'\n'inputs':\n- 'name': 'input workflow2 from MD Setup'\n'section': '../uploads/ME2oYBdiQUW4CGcG0YsGuw/archive/8EcTSRbvb8PiYaG_TwqAgOYiE8J7#/workflow2'\n'outputs':\n- 'name': 'output workflow2 from MD Equilibration'\n'section': '../uploads/7Ncu4YyXTTyTSfg0qpiziQ/archive/J3Vkhz2NAtSw4-KTnbHqUyPhhY3g#/workflow2'\n- 'm_def': 'nomad.datamodel.metainfo.workflow.TaskReference'\n'name': 'DFT-1'\n'task': '../uploads/JHsC4UFnRtCB9dQYR1BO6A/archive/d2ZJkTjL4LoxdFAVS8YT_jlZdzhm#/workflow2'\n'inputs':\n- 'name': 'input workflow2 from MD Equilibration'\n'section': '../uploads/7Ncu4YyXTTyTSfg0qpiziQ/archive/J3Vkhz2NAtSw4-KTnbHqUyPhhY3g#/workflow2'\n- 'name': 'input system from DFT-1'\n'section': '../uploads/JHsC4UFnRtCB9dQYR1BO6A/archive/d2ZJkTjL4LoxdFAVS8YT_jlZdzhm#/run/0/system/-1'\n'outputs':\n- 'name': 'output data from Vibrational Analysis'\n'section': '../uploads/yyqHpBFOSxqhrNALIJswSw/archive/zv4Q_jnvhsGry4oEyj5AC2qD103H#/data'\n- 'name': 'output system from DFT-1'\n'section': '../uploads/JHsC4UFnRtCB9dQYR1BO6A/archive/d2ZJkTjL4LoxdFAVS8YT_jlZdzhm#/run/0/system/-1'\n- 'name': 'output calculation from DFT-1'\n'section': '../uploads/JHsC4UFnRtCB9dQYR1BO6A/archive/d2ZJkTjL4LoxdFAVS8YT_jlZdzhm#/run/0/calculation/-1'\n- 'm_def': 'nomad.datamodel.metainfo.workflow.TaskReference'\n'name': 'DFT-2'\n'task': '../uploads/exkqL5J8Q62ldMHbuE5tcQ/archive/zeiiGOoZL0dikRgYSvzuZZ3xPv9m#/workflow2'\n'inputs':\n- 'name': 'input workflow2 from MD Equilibration'\n'section': '../uploads/7Ncu4YyXTTyTSfg0qpiziQ/archive/J3Vkhz2NAtSw4-KTnbHqUyPhhY3g#/workflow2'\n- 'name': 'input system from DFT-2'\n'section': '../uploads/exkqL5J8Q62ldMHbuE5tcQ/archive/zeiiGOoZL0dikRgYSvzuZZ3xPv9m#/run/0/system/-1'\n'outputs':\n- 'name': 'output data from Vibrational Analysis'\n'section': '../uploads/yyqHpBFOSxqhrNALIJswSw/archive/zv4Q_jnvhsGry4oEyj5AC2qD103H#/data'\n- 'name': 'output system from DFT-2'\n'section': '../uploads/exkqL5J8Q62ldMHbuE5tcQ/archive/zeiiGOoZL0dikRgYSvzuZZ3xPv9m#/run/0/system/-1'\n- 'name': 'output calculation from DFT-2'\n'section': '../uploads/exkqL5J8Q62ldMHbuE5tcQ/archive/zeiiGOoZL0dikRgYSvzuZZ3xPv9m#/run/0/calculation/-1'\n- 'm_def': 'nomad.datamodel.metainfo.workflow.TaskReference'\n'name': 'DFT-3'\n'task': '../uploads/0wUg7_GuSLiG3U1LcNlr2A/archive/8x9j0tEtx6LeGVEv7ByWD3k5VK1F#/workflow2'\n'inputs':\n- 'name': 'input workflow2 from MD Equilibration'\n'section': '../uploads/7Ncu4YyXTTyTSfg0qpiziQ/archive/J3Vkhz2NAtSw4-KTnbHqUyPhhY3g#/workflow2'\n- 'name': 'input system from DFT-3'\n'section': '../uploads/0wUg7_GuSLiG3U1LcNlr2A/archive/8x9j0tEtx6LeGVEv7ByWD3k5VK1F#/run/0/system/-1'\n'outputs':\n- 'name': 'output data from Vibrational Analysis'\n'section': '../uploads/yyqHpBFOSxqhrNALIJswSw/archive/zv4Q_jnvhsGry4oEyj5AC2qD103H#/data'\n- 'name': 'output system from DFT-3'\n'section': '../uploads/0wUg7_GuSLiG3U1LcNlr2A/archive/8x9j0tEtx6LeGVEv7ByWD3k5VK1F#/run/0/system/-1'\n- 'name': 'output calculation from DFT-3'\n'section': '../uploads/0wUg7_GuSLiG3U1LcNlr2A/archive/8x9j0tEtx6LeGVEv7ByWD3k5VK1F#/run/0/calculation/-1'\n</code></pre> <p>Now upload, edit the metadata, and publish <code>project_workflow.archive.yaml</code> following Part 3 &gt; Uploading and Publishing. Browse the workflow graph of this entry to see how it links all of your uploads together. Your workflow graph should look like this:</p> <p>Now, go to <code>PUBLISH &gt; Datasets</code> and find the dataset that you created. Click the arrow to the right of this dataset and browse all the entries that it contains to make sure all of your uploads are included. Then, go back to the datasets page, and click the \"assign a DOI\" icon to publish your dataset.</p> <p>That's it! You now have a persistant identifier to add to your publication in order to reference your data. Once your manuscript is accepted and receives a DOI, you can then cross-reference the manuscript by once again editing the metadata of each upload to include a reference to your paper.</p>"}]}